id: gcp_spark_bq
namespace: ghg_project
description: "Job to upload pyspark script to gcs bucket, submit job to dataproc and create a table in BigQuery"

tasks:

  - id: submit_pyspark_job
    type: io.kestra.plugin.gcp.cli.GCloudCLI
    projectId: "{{kv('GCP_PROJECT_ID')}}"
    serviceAccount: "{{kv('GCP_CREDS')}}"
    commands: 
    - gcloud dataproc jobs submit pyspark gs://{{kv('GCP_BUCKET_NAME')}}/scripts/transform_ghg_data.py --cluster="{{kv('GCP_CLUSTER')}}" --region="{{kv('GCP_REGION')}}" --project="{{kv('GCP_PROJECT_ID')}}"
    retry:
      type: constant
      maxAttempt: 2  
      interval: PT50S

  - id: create_bq_table
    type: io.kestra.plugin.gcp.bigquery.Query
    sql: |
      LOAD DATA INTO `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_DATASET')}}.emissions_data`
      FROM FILES (
        format = 'PARQUET',
        uris = ['gs://{{kv('GCP_BUCKET_NAME')}}/processed/emissions_data/*.parquet']
      );

  - id: create_bq_external_table
    type: io.kestra.plugin.gcp.bigquery.Query
    sql: |
      CREATE OR REPLACE EXTERNAL TABLE `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_DATASET')}}.external_emissions_data`
      OPTIONS (
        format = 'PARQUET',
        uris = ['gs://{{kv('GCP_BUCKET_NAME')}}/processed/emissions_data/*.parquet']
      );
      
  - id: purge_files
    type: io.kestra.plugin.core.storage.PurgeCurrentExecutionFiles
    description: To avoid cluttering your storage, we will remove the downloaded files

pluginDefaults:
  - type: io.kestra.plugin.gcp
    values:
      serviceAccount: "{{kv('GCP_CREDS')}}"
      projectId: "{{kv('GCP_PROJECT_ID')}}"
      location: "{{kv('GCP_LOCATION')}}"
      bucket: "{{kv('GCP_BUCKET_NAME')}}"
      dataset: "{{kv('GCP_DATASET')}}"
      cluster: "{{kv('GCP_CLUSTER')}}"
      region: "{{kv('GCP_REGION')}}"

triggers:
  - id: yearly_schedule
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "0 12 1 1 *"